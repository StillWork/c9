{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gg-50-MNIST-텐서플로우.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StillWork/ds-lab/blob/master/gg_50_MNIST_%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T7-pAFnVaM1G",
        "colab_type": "code",
        "outputId": "f7943130-a0c0-47f6-8778-e0921477be3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4200
        }
      },
      "cell_type": "code",
      "source": [
        "# 단층 퍼셉트론으로 구축하기\n",
        "# 성능이 80% 정도 나온다\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "#원핫 인코딩을 선택한다\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 200\n",
        "\n",
        "# None은 임의의 크기를 담을 수 있다는 것을 말한다\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
        "\n",
        "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
        "bias = tf.Variable(tf.random_normal([no_classes]))\n",
        "\n",
        "logits = tf.matmul(x_input, weights) + bias\n",
        "\n",
        "# 크로스엔트로피와 소프트맥스를 사용한다\n",
        "# 손실함수와, 최적화기를 선택\n",
        "# 학습속도 0.5\n",
        "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    labels=y_input, logits=logits)\n",
        "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
        "optimiser = tf.train.GradientDescentOptimizer(\n",
        "    learning_rate=0.5).minimize(loss_operation)\n",
        "\n",
        "# 초기화\n",
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "# 배치 작업 수행 \n",
        "# feed_dict 인자에 훈련 데이터 딕셔너이를 공급\n",
        "for batch_no in range(total_batches):\n",
        "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
        "        x_input: train_images,\n",
        "        y_input: train_labels\n",
        "    })\n",
        "    print(loss_value)\n",
        "\n",
        "predictions = tf.argmax(logits, 1)\n",
        "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions,\n",
        "                                            tf.float32))\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
        "    x_input: test_images,\n",
        "    y_input: test_labels\n",
        "})\n",
        "print('Accuracy : ', accuracy_value)\n",
        "session.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-33e361e75dce>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-33e361e75dce>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "11.836988\n",
            "9.359686\n",
            "9.28421\n",
            "7.5117016\n",
            "7.4790735\n",
            "6.566105\n",
            "7.6782966\n",
            "6.4308305\n",
            "6.448367\n",
            "6.1576433\n",
            "4.8656616\n",
            "4.7898293\n",
            "4.483579\n",
            "4.20174\n",
            "4.1258655\n",
            "4.6582813\n",
            "4.4721336\n",
            "3.0129108\n",
            "4.4249654\n",
            "3.4985452\n",
            "3.5047164\n",
            "4.146749\n",
            "3.2995682\n",
            "3.4070861\n",
            "3.710599\n",
            "2.6715448\n",
            "2.666792\n",
            "3.6414742\n",
            "2.8883169\n",
            "2.939517\n",
            "2.9623454\n",
            "2.3070154\n",
            "2.3649933\n",
            "2.8601716\n",
            "2.8708735\n",
            "2.626026\n",
            "2.7838688\n",
            "3.119657\n",
            "2.9108117\n",
            "2.0327232\n",
            "2.7014086\n",
            "1.7578559\n",
            "2.4306977\n",
            "2.0313325\n",
            "2.1964116\n",
            "2.2014213\n",
            "3.1231842\n",
            "2.7343702\n",
            "2.2608259\n",
            "1.6946952\n",
            "2.3162541\n",
            "1.4764276\n",
            "2.0376296\n",
            "2.3350363\n",
            "2.0671482\n",
            "2.1941593\n",
            "1.6841545\n",
            "2.283525\n",
            "2.1697717\n",
            "1.8461877\n",
            "1.6622609\n",
            "2.6492028\n",
            "1.2925961\n",
            "1.6009688\n",
            "1.3378886\n",
            "1.8374298\n",
            "1.4779363\n",
            "2.224336\n",
            "2.0285053\n",
            "1.441421\n",
            "1.8442808\n",
            "1.4254838\n",
            "2.443585\n",
            "2.0862093\n",
            "1.8244995\n",
            "1.8008264\n",
            "1.6028517\n",
            "2.2830353\n",
            "2.8366134\n",
            "1.6723964\n",
            "1.7852299\n",
            "1.4972241\n",
            "1.8514502\n",
            "1.5074276\n",
            "1.465303\n",
            "1.6617818\n",
            "1.6016312\n",
            "1.9811064\n",
            "2.0293527\n",
            "1.7004178\n",
            "1.6978014\n",
            "1.6339444\n",
            "1.77888\n",
            "1.6282927\n",
            "2.2411678\n",
            "2.1864638\n",
            "1.2520484\n",
            "2.0233948\n",
            "1.3192091\n",
            "0.99868196\n",
            "1.1017358\n",
            "1.687767\n",
            "1.3323832\n",
            "1.0330254\n",
            "1.0452654\n",
            "1.213105\n",
            "1.2929122\n",
            "1.8809048\n",
            "1.4166064\n",
            "1.425911\n",
            "1.374121\n",
            "1.3122274\n",
            "1.1597142\n",
            "0.93958986\n",
            "1.3494401\n",
            "1.1268889\n",
            "1.1669813\n",
            "1.4227414\n",
            "1.2690481\n",
            "1.6309752\n",
            "1.5272156\n",
            "0.97550887\n",
            "1.3502367\n",
            "1.7893871\n",
            "0.92939955\n",
            "1.0707006\n",
            "1.3904021\n",
            "1.4178582\n",
            "1.6466107\n",
            "1.4574116\n",
            "0.84499604\n",
            "1.2936678\n",
            "1.5138327\n",
            "1.4743081\n",
            "1.2803557\n",
            "1.4217236\n",
            "1.1543022\n",
            "1.1407055\n",
            "1.3638057\n",
            "1.0740514\n",
            "1.1226346\n",
            "1.0062344\n",
            "0.8607431\n",
            "0.9249361\n",
            "0.7863641\n",
            "1.2374433\n",
            "1.0969046\n",
            "1.0621102\n",
            "1.321159\n",
            "1.2556164\n",
            "1.0940568\n",
            "0.96432215\n",
            "1.6507317\n",
            "1.4717376\n",
            "0.9957112\n",
            "1.1250134\n",
            "0.9124446\n",
            "1.4452115\n",
            "0.7415254\n",
            "1.2866968\n",
            "0.91768616\n",
            "1.2787622\n",
            "0.92282075\n",
            "1.2298284\n",
            "0.81876665\n",
            "1.4195\n",
            "1.2687951\n",
            "1.2776037\n",
            "0.9835011\n",
            "1.1817899\n",
            "1.4175506\n",
            "1.0589535\n",
            "1.4001323\n",
            "1.1243864\n",
            "1.1368018\n",
            "0.7959678\n",
            "1.1485019\n",
            "1.1708294\n",
            "1.5278254\n",
            "1.2217607\n",
            "0.8243078\n",
            "0.6283525\n",
            "1.097764\n",
            "1.5282472\n",
            "1.4255877\n",
            "0.8641308\n",
            "0.98464715\n",
            "0.6350443\n",
            "1.350009\n",
            "1.3742245\n",
            "0.8837707\n",
            "1.4210743\n",
            "0.8673749\n",
            "1.44908\n",
            "0.872573\n",
            "0.9243496\n",
            "0.995353\n",
            "1.4393599\n",
            "1.0412256\n",
            "1.024004\n",
            "Accuracy :  0.8071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hX-S-F1plDSs",
        "colab_type": "code",
        "outputId": "a5a5e071-e3a1-430d-f069-cbf802389fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "cell_type": "code",
      "source": [
        "# CNN으로 구축하기\n",
        "# 텐서플로우의 layer API를 사용\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 200\n",
        "\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
        "\n",
        "\n",
        "def add_variable_summary(tf_variable, summary_name):\n",
        "  with tf.name_scope(summary_name + '_summary'):\n",
        "    mean = tf.reduce_mean(tf_variable)\n",
        "    tf.summary.scalar('Mean', mean)\n",
        "    with tf.name_scope('standard_deviation'):\n",
        "        standard_deviation = tf.sqrt(tf.reduce_mean(\n",
        "            tf.square(tf_variable - mean)))\n",
        "    tf.summary.scalar('StandardDeviation', standard_deviation)\n",
        "    tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
        "    tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
        "    tf.summary.histogram('Histogram', tf_variable)\n",
        "\n",
        "\n",
        "x_input_reshape = tf.reshape(x_input, [-1, 28, 28, 1],\n",
        "                             name='input_reshape')\n",
        "\n",
        "\n",
        "def convolution_layer(input_layer, filters, kernel_size=[3, 3],\n",
        "                      activation=tf.nn.relu):\n",
        "    layer = tf.layers.conv2d(\n",
        "        inputs=input_layer,\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        activation=activation\n",
        "    )\n",
        "    add_variable_summary(layer, 'convolution')\n",
        "    return layer\n",
        "\n",
        "\n",
        "def pooling_layer(input_layer, pool_size=[2, 2], strides=2):\n",
        "    layer = tf.layers.max_pooling2d(\n",
        "        inputs=input_layer,\n",
        "        pool_size=pool_size,\n",
        "        strides=strides\n",
        "    )\n",
        "    add_variable_summary(layer, 'pooling')\n",
        "    return layer\n",
        "\n",
        "\n",
        "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
        "    layer = tf.layers.dense(\n",
        "        inputs=input_layer,\n",
        "        units=units,\n",
        "        activation=activation\n",
        "    )\n",
        "    add_variable_summary(layer, 'dense')\n",
        "    return layer\n",
        "\n",
        "\n",
        "convolution_layer_1 = convolution_layer(x_input_reshape, 64)\n",
        "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
        "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
        "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
        "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5 * 5 * 128],\n",
        "                            name='flattened_pool')\n",
        "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)\n",
        "\n",
        "dropout_bool = tf.placeholder(tf.bool)\n",
        "dropout_layer = tf.layers.dropout(\n",
        "        inputs=dense_layer_bottleneck,\n",
        "        rate=0.4,\n",
        "        training=dropout_bool\n",
        "    )\n",
        "logits = dense_layer(dropout_layer, no_classes)\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=y_input, logits=logits)\n",
        "    loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
        "    tf.summary.scalar('loss', loss_operation)\n",
        "\n",
        "with tf.name_scope('optimiser'):\n",
        "    optimiser = tf.train.AdamOptimizer().minimize(loss_operation)\n",
        "\n",
        "\n",
        "with tf.name_scope('accuracy'):\n",
        "    with tf.name_scope('correct_prediction'):\n",
        "        predictions = tf.argmax(logits, 1)\n",
        "        correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "    with tf.name_scope('accuracy'):\n",
        "        accuracy_operation = tf.reduce_mean(\n",
        "            tf.cast(correct_predictions, tf.float32))\n",
        "tf.summary.scalar('accuracy', accuracy_operation)\n",
        "\n",
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "merged_summary_operation = tf.summary.merge_all()\n",
        "train_summary_writer = tf.summary.FileWriter('/tmp/train', session.graph)\n",
        "test_summary_writer = tf.summary.FileWriter('/tmp/test')\n",
        "\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "\n",
        "for batch_no in range(total_batches):\n",
        "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "    _, merged_summary = session.run([optimiser, merged_summary_operation],\n",
        "                                    feed_dict={\n",
        "        x_input: train_images,\n",
        "        y_input: train_labels,\n",
        "        dropout_bool: True\n",
        "    })\n",
        "    train_summary_writer.add_summary(merged_summary, batch_no)\n",
        "    if batch_no % 10 == 0:\n",
        "        merged_summary, _ = session.run([merged_summary_operation,\n",
        "                                         accuracy_operation], feed_dict={\n",
        "            x_input: test_images,\n",
        "            y_input: test_labels,\n",
        "            dropout_bool: False\n",
        "        })\n",
        "        test_summary_writer.add_summary(merged_summary, batch_no)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:49: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:59: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:77: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-1-33214579b93e>:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bEVRj1tkpTW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}